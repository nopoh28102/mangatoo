import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import random
import re

class SimpleMangaDownloader:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://olympustaff.com/'
        })
    
    def scrape_and_download_chapter(self, chapter_url, output_folder):
        """
        ÙƒØ´Ø· ÙˆØªØ­Ù…ÙŠÙ„ ÙØµÙ„ ÙƒØ§Ù…Ù„ - Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¨Ø³Ø·Ø© Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
        """
        try:
            os.makedirs(output_folder, exist_ok=True)
            
            print(f"ğŸ” Ø¬Ø§Ø±ÙŠ ÙƒØ´Ø· Ø§Ù„ÙØµÙ„ Ù…Ù†: {chapter_url}")
            
            response = self.session.get(chapter_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            chapter_title = self.extract_chapter_title(soup)
            print(f"ğŸ“– Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙØµÙ„: {chapter_title}")
            
            image_urls = self.extract_image_urls(soup)
            
            if not image_urls:
                print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ØµÙˆØ± ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙØµÙ„!")
                return {
                    'success': False,
                    'error': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ±',
                    'chapter_title': chapter_title,
                    'images_found': 0,
                    'downloaded_files': []
                }
            
            print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(image_urls)} ØµÙˆØ±Ø©")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
            downloaded_files = []
            for i, img_url in enumerate(image_urls):
                try:
                    print(f"â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© {i+1}/{len(image_urls)}")
                    
                    img_response = self.session.get(img_url, stream=True, headers={'Referer': chapter_url})
                    img_response.raise_for_status()
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„ØµØ­ÙŠØ­
                    file_extension = self.get_file_extension(img_url)
                    filename = f"page_{i+1:03d}{file_extension}"
                    img_path = os.path.join(output_folder, filename)
                    
                    with open(img_path, 'wb') as f:
                        for chunk in img_response.iter_content(chunk_size=8192):
                            f.write(chunk)
                    
                    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
                    file_size = os.path.getsize(img_path)
                    if file_size > 1000:  # Ø£ÙƒØ¨Ø± Ù…Ù† 1KB
                        downloaded_files.append(img_path)
                        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„: {filename} ({file_size} bytes)")
                    else:
                        print(f"âš ï¸ Ù…Ù„Ù ØµØºÙŠØ±ØŒ Ø³ÙŠØªÙ… Ø­Ø°ÙÙ‡: {filename}")
                        os.remove(img_path)
                    
                    # ØªØ£Ø®ÙŠØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
                    time.sleep(random.uniform(0.1, 0.3))
                
                except Exception as img_error:
                    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© {i+1}: {img_error}")
            
            print(f"ğŸ¯ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(downloaded_files)} Ù…Ù† Ø£ØµÙ„ {len(image_urls)} ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­")
            
            return {
                'success': True,
                'error': None,
                'chapter_title': chapter_title,
                'images_found': len(image_urls),
                'downloaded_files': downloaded_files
            }
            
        except Exception as e:
            print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")
            return {
                'success': False,
                'error': str(e),
                'chapter_title': None,
                'images_found': 0,
                'downloaded_files': []
            }
    
    def extract_chapter_title(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙØµÙ„"""
        title_tag = soup.find('h1', class_='entry-title')
        if title_tag:
            return title_tag.text.strip()
        
        # Ø¬Ø±Ø¨ Ù…Ø­Ø¯Ø¯Ø§Øª Ø£Ø®Ø±Ù‰
        for selector in ['h1', 'h2', '.title', '.chapter-title']:
            title_element = soup.select_one(selector)
            if title_element:
                return title_element.get_text().strip()
        
        return "ÙØµÙ„ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
    
    def extract_image_urls(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ± Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        images = []
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ div.entry-content img ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø±ÙÙ‚
        img_containers = soup.select('div.entry-content img')
        
        for img in img_containers:
            img_url = img.get('src') or img.get('data-src') or img.get('data-original')
            if img_url and not img_url.startswith('data:'):
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ù…Ø·Ù„Ù‚Ø©
                if 'olympustaff.com' not in img_url:
                    img_url = urljoin('https://olympustaff.com/', img_url)
                
                # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙˆØ±Ø©
                if any(ext in img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                    images.append(img_url)
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ ØµÙˆØ± ÙÙŠ entry-contentØŒ Ø¬Ø±Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ±
        if not images:
            all_images = soup.find_all('img')
            for img in all_images:
                img_url = img.get('src') or img.get('data-src')
                if img_url and 'olympustaff.com' in img_url:
                    if any(ext in img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                        images.append(img_url)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        unique_images = []
        seen_urls = set()
        for img_url in images:
            if img_url not in seen_urls:
                unique_images.append(img_url)
                seen_urls.add(img_url)
        
        return unique_images
    
    def get_file_extension(self, url):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
        if '.webp' in url.lower():
            return '.webp'
        elif '.png' in url.lower():
            return '.png'
        elif '.jpg' in url.lower() or '.jpeg' in url.lower():
            return '.jpg'
        else:
            return '.jpg'  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ routes.py
def scrape_olympustaff_simple(chapter_url, output_folder):
    """
    Ø¯Ø§Ù„Ø© Ù…Ø¨Ø³Ø·Ø© Ù„ÙƒØ´Ø· ÙˆØªØ­Ù…ÙŠÙ„ ÙØµÙ„ Ù…Ù† olympustaff
    """
    downloader = SimpleMangaDownloader()
    return downloader.scrape_and_download_chapter(chapter_url, output_folder)

def test_olympustaff_scraping(chapter_url):
    """
    Ø¯Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒØ´Ø· Ø¨Ø¯ÙˆÙ† ØªØ­Ù…ÙŠÙ„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ /admin/test-scrape
    """
    try:
        downloader = SimpleMangaDownloader()
        
        print(f"ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒØ´Ø· Ù…Ù†: {chapter_url}")
        
        response = downloader.session.get(chapter_url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        chapter_title = downloader.extract_chapter_title(soup)
        image_urls = downloader.extract_image_urls(soup)
        
        if not image_urls:
            return {
                'success': False,
                'error': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ØµÙˆØ± ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙØµÙ„'
            }
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙƒØªØ´ÙØ©
        return {
            'success': True,
            'chapter_title': chapter_title or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
            'total_images': len(image_urls),
            'sample_images': image_urls[:6],  # Ø£ÙˆÙ„ 6 ØµÙˆØ± Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø±ÙŠØ¹
            'all_images': image_urls  # Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ù„Ù„Ø±ÙØ¹ Ø§Ù„ÙØ¹Ù„ÙŠ
        }
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒØ´Ø·: {e}")
        return {
            'success': False,
            'error': f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {str(e)}'
        }

# Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
if __name__ == "__main__":
    downloader = SimpleMangaDownloader()
    
    chapter_url = input("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„ÙØµÙ„ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ ØªÙ†Ø²ÙŠÙ„Ù‡: ")
    output_folder = "downloads"
    
    result = downloader.scrape_and_download_chapter(chapter_url, output_folder)
    
    if result['success']:
        print(f"\nğŸ‰ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"ğŸ“ Ø§Ù„Ù…Ø¬Ù„Ø¯: {output_folder}")
        print(f"ğŸ“„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(result['downloaded_files'])}")
    else:
        print(f"\nğŸ’” ÙØ´Ù„: {result['error']}")